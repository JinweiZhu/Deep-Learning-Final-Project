{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WtpAa-ruWjQE"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import copy\n",
    "\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hNCVUQ7vWjQG",
    "outputId": "c43ca146-94b0-4262-c826-145552b8fe97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ed2BjBXQWjQG"
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "num_frames = 22\n",
    "\n",
    "device_ids = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# list all directories in the current directory\n",
    "directories = glob.glob(\"/scratch/pt2295/Dataset_Student/train/video_*\")#[d for d in os.listdir('/scratch/pt2295/Dataset_Student/train/') if os.path.isdir(d)]\n",
    "\n",
    "# create an empty list to store numpy arrays\n",
    "arrays = {}\n",
    "# print(directories)\n",
    "\n",
    "# loop through each directory\n",
    "ctr=0\n",
    "for d in directories:\n",
    "    # construct the path to the mean.npy file\n",
    "    path_to_mean = os.path.join(d, 'mask.npy')\n",
    "    # read the mean.npy file as a numpy array\n",
    "    mean_array = np.load(path_to_mean)\n",
    "    print(mean_array.shape)\n",
    "    # append the mean array to the list of arrays\n",
    "    arrays[ctr]=mean_array\n",
    "    ctr+=1\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(mask, num_classes=49):\n",
    "  # mask: a numpy array of shape (batch, height, width) with integer class labels\n",
    "  # num_classes: an integer representing the number of classes\n",
    "  # returns: a numpy array of shape (batch, height, width, num_classes) with one-hot encoded labels\n",
    "    one_hot = np.eye(num_classes)[mask] # select rows from identity matrix\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MeanNpyDataset(Dataset):\n",
    "    def __init__(self, directory_path):\n",
    "        self.directory_path = directory_path\n",
    "        self.folder_names = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = self.folder_names[idx]\n",
    "       \n",
    "        path_to_mean = os.path.join(self.directory_path, folder_name, 'mask.npy')\n",
    "\n",
    "        mean_array = np.load(path_to_mean)\n",
    "       \n",
    "        mean_array=one_hot_encoder(mean_array)\n",
    "        \n",
    "\n",
    "        return torch.from_numpy(mean_array), folder_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeanNpyDataset('/scratch/jz3395/train')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 22, 160, 240, 49])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (mean_tensor, folder_name) in enumerate(dataloader):\n",
    "    # do something with the batch of mean tensors and corresponding folder names\n",
    "#     print(f\"Batch {batch_idx}, folder names: {folder_name}\")\n",
    "    print(mean_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xa7OlEJbWjQH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "    kernel_size, padding, activation, frame_size):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()  \n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = torch.tanh \n",
    "        elif activation == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "        \n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels + out_channels, \n",
    "            out_channels=4 * out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            padding=padding)           \n",
    "\n",
    "        # Initialize weights for Hadamard Products\n",
    "        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "\n",
    "    def forward(self, X, H_prev, C_prev):\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
    "\n",
    "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n",
    "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n",
    "\n",
    "        # Current Cell output\n",
    "        C = forget_gate*C_prev + input_gate * self.activation(C_conv)\n",
    "\n",
    "        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n",
    "\n",
    "        # Current Hidden State\n",
    "        H = output_gate * self.activation(C)\n",
    "\n",
    "        return H, C\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "    kernel_size, padding, activation, frame_size):\n",
    "\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # We will unroll this over time steps\n",
    "        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, \n",
    "        kernel_size, padding, activation, frame_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n",
    "\n",
    "        # Get the dimensions\n",
    "        batch_size, _, seq_len, height, width = X.size()\n",
    "\n",
    "        # Initialize output\n",
    "        output = torch.zeros(batch_size, self.out_channels, seq_len, \n",
    "        height, width, device=device)\n",
    "        \n",
    "        # Initialize Hidden State\n",
    "        H = torch.zeros(batch_size, self.out_channels, \n",
    "        height, width, device=device)\n",
    "\n",
    "        # Initialize Cell Input\n",
    "        C = torch.zeros(batch_size,self.out_channels, \n",
    "        height, width, device=device)\n",
    "\n",
    "        # Unroll over time steps\n",
    "        for time_step in range(seq_len):\n",
    "\n",
    "            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n",
    "\n",
    "            output[:,:,time_step] = H\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n",
    "    activation, frame_size, num_layers):\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\", ConvLSTM(\n",
    "                in_channels=num_channels, out_channels=num_kernels,\n",
    "                kernel_size=kernel_size, padding=padding, \n",
    "                activation=activation, frame_size=frame_size)\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        ) \n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers+1):\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\", ConvLSTM(\n",
    "                    in_channels=num_kernels, out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size, padding=padding, \n",
    "                    activation=activation, frame_size=frame_size)\n",
    "                )\n",
    "                \n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "                ) \n",
    "\n",
    "        # Add Convolutional Layer to predict output frame\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels, out_channels=num_channels,\n",
    "            kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Forward propagation through all the layers\n",
    "        output = self.sequential(X)\n",
    "\n",
    "        # Return only the last output frame\n",
    "        output = self.conv(output[:,:,-1])\n",
    "        \n",
    "        return nn.Softmax()(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 48])\n",
      "tensor([[ 1., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90.,\n",
      "         90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90.,\n",
      "         90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90.,\n",
      "         90., 90., 90., 90., 90., 90., 90.]])\n"
     ]
    }
   ],
   "source": [
    "class_weight=torch.eye(1,1)\n",
    "print(class_weight.size())\n",
    "full=torch.full((1,48),49)\n",
    "print(full.size())\n",
    "class_weight=torch.cat((class_weight,full),dim=1)\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "64F0SWFtWjQI"
   },
   "outputs": [],
   "source": [
    "# ckpt = torch.load('convlstm_predictor.pth')\n",
    "# predictor.load_state_dict(ckpt)\n",
    "\n",
    "predictor = Seq2Seq(num_channels=49, num_kernels=64, kernel_size=(3, 3), padding=(1, 1), activation=\"relu\", \n",
    "                frame_size=(160, 240), num_layers=3)\n",
    "\n",
    "predictor = nn.DataParallel(predictor.cuda(), device_ids=device_ids).cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(predictor.parameters(), lr=1e-2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weight.to(device),reduction='mean')\n",
    "starting_epoch=89\n",
    "checkpoint = torch.load('/scratch/jz3395/p_convlstm_predictor'+str(starting_epoch)+'.pth')\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=5)\n",
    "predictor.load_state_dict(checkpoint)\n",
    "jaccard= torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=49).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_11_frames_input():\n",
    "    num_epochs = 60\n",
    "    t1 = datetime.now()\n",
    "    for e in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        correct_last_frame_prediction=0\n",
    "        total_last_frame_prediction=0\n",
    "        jaccard_index=0\n",
    "        for i, (true_masks, folder_name) in enumerate(dataloader):\n",
    "            print(i,end=' ')\n",
    "            #print(\"avg IoU last image:\",jaccard_index)\n",
    "            optimizer.zero_grad()\n",
    "            true_masks=true_masks.to(torch.float32).permute(0,4,1,2,3).to(device)\n",
    "            #true_masks is of dimension here:([16, 49, 22, 160, 240])\n",
    "            first_11_true_frames = true_masks[:,:,:11,:,:]\n",
    "            last_11_true_frames=true_masks[:,:,11:,:,:]\n",
    "            pred_mask=predictor(first_11_true_frames)\n",
    "            \n",
    "            for i in range(0,11):\n",
    "                #pred mask is of dimension torch.Size([16, 49, 160, 240])\n",
    "                frame_of_true_mask=true_masks.argmax(dim=1).squeeze()[:, 11+i, :,:]\n",
    "                loss = criterion(pred_mask,frame_of_true_mask)\n",
    "                new_correct=torch.sum(pred_mask.argmax(dim=1).squeeze()==frame_of_true_mask)\n",
    "                new_total=batch_size*160*240\n",
    "                correct+=new_correct\n",
    "                total+=new_total\n",
    "                if i==10:\n",
    "                    total_last_frame_prediction+=new_total\n",
    "                    correct_last_frame_prediction+=new_correct\n",
    "                    #jaccard_index=jaccard(pred_mask.argmax(dim=1).squeeze(),frame_of_true_mask)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                if i!=10:\n",
    "                    next_mask = last_11_true_frames[:, :, i, :, :]\n",
    "                    next_mask=next_mask.unsqueeze(2)\n",
    "                    pred_mask= predictor(next_mask)\n",
    "            \n",
    "        total_epoch=e+starting_epoch\n",
    "        torch.save(predictor.state_dict(), '/scratch/jz3395/p_convlstm_predictor'+str(total_epoch+1)+'.pth')\n",
    "        \n",
    "        print(f\"\\n\\nEpoch: {total_epoch}, Loss: {total_loss}\\n, overall prediction accuracy: {correct/total}\")\n",
    "        print(f\"\\n\\nEpoch: {total_epoch}, Prediction accuracy of last frame: {correct_last_frame_prediction/total_last_frame_prediction}\")\n",
    "    print(f\"Training took time: {datetime.now() - t1}\")\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook already signed: convlstm.ipynb\n"
     ]
    }
   ],
   "source": [
    "!jupyter trust convlstm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-32887502/ipykernel_2858881/530924047.py:138: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.Softmax()(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \n",
      "\n",
      "Epoch: 89, Loss: 1515.6507575511932\n",
      ", overall prediction accuracy: 0.934627115726471\n",
      "\n",
      "\n",
      "Epoch: 89, Prediction accuracy of last frame: 0.9425966739654541\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \n",
      "\n",
      "Epoch: 90, Loss: 1556.3480298519135\n",
      ", overall prediction accuracy: 0.9359146952629089\n",
      "\n",
      "\n",
      "Epoch: 90, Prediction accuracy of last frame: 0.9451081156730652\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \n",
      "\n",
      "Epoch: 91, Loss: 1537.0702075958252\n",
      ", overall prediction accuracy: 0.9387143850326538\n",
      "\n",
      "\n",
      "Epoch: 91, Prediction accuracy of last frame: 0.9446868300437927\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 "
     ]
    }
   ],
   "source": [
    "train_for_11_frames_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeanNpyDataset('/scratch/jz3395/train')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "def validate_for_11_frames_input():\n",
    "    num_epochs = 50\n",
    "    t1 = datetime.now()\n",
    "    for e in range(1):\n",
    "        total_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        correct_last_frame_prediction=0\n",
    "        total_last_frame_prediction=0\n",
    "        jaccard_index=0\n",
    "        for i, (true_masks, folder_name) in enumerate(dataloader):\n",
    "            print(i,end=' ')\n",
    "            print(\"avg IoU last image:\",jaccard_index)\n",
    "            optimizer.zero_grad()\n",
    "            true_masks=true_masks.to(torch.float32).permute(0,4,1,2,3).to(device)\n",
    "            #true_masks is of dimension here:([16, 49, 22, 160, 240])\n",
    "            first_11_true_frames = true_masks[:,:,:11,:,:]\n",
    "            last_11_true_frames=true_masks[:,:,11:,:,:]\n",
    "            pred_mask=predictor(first_11_true_frames)\n",
    "            for i in range(0,11):\n",
    "                #pred mask is of dimension torch.Size([16, 49, 160, 240])\n",
    "                frame_of_true_mask=true_masks.argmax(dim=1).squeeze()[:, 11+i, :,:]\n",
    "                loss = criterion(pred_mask,frame_of_true_mask)\n",
    "                new_correct=torch.sum(pred_mask.argmax(dim=1).squeeze()==frame_of_true_mask)\n",
    "                new_total=batch_size*160*240\n",
    "                correct+=new_correct\n",
    "                total+=new_total\n",
    "                if i==10:\n",
    "                    total_last_frame_prediction+=new_total\n",
    "                    correct_last_frame_prediction+=new_correct\n",
    "                    jaccard_index=jaccard(pred_mask.argmax(dim=1).squeeze(),frame_of_true_mask)\n",
    "                total_loss += loss.item()\n",
    "                if i!=10:\n",
    "                    next_mask = last_11_true_frames[:, :, i, :, :]\n",
    "                    next_mask=next_mask.unsqueeze(2)\n",
    "                    pred_mask= predictor(next_mask)\n",
    "            \n",
    "        total_epoch=e+starting_epoch\n",
    "        print(f\"\\n\\nEpoch: {total_epoch}, Loss: {total_loss}\\n, overall prediction accuracy: {correct/total}\")\n",
    "        print(f\"\\n\\nEpoch: {total_epoch}, Prediction accuracy of last frame: {correct_last_frame_prediction/total_last_frame_prediction}\")\n",
    "    print(f\"Training took time: {datetime.now() - t1}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(pred, actual):\n",
    "\n",
    "    pred = pred.argmax(dim=1)\n",
    "\n",
    "    B = pred.shape[0]\n",
    "\n",
    "    fig, ax = plt.subplots(B // 2, 4, figsize=(160, 240))\n",
    "\n",
    "    for i in range(B // 2):\n",
    "        ax[i][0].title.set_text(\"Actual\")\n",
    "        ax[i][0].imshow(actual[2*i].cpu())\n",
    "        \n",
    "        ax[i][1].title.set_text(\"Predicted\")\n",
    "        ax[i][1].imshow(pred[2*i].cpu())\n",
    "\n",
    "        ax[i][2].title.set_text(\"Actual\")\n",
    "        ax[i][2].imshow(actual[2*i+1].cpu())\n",
    "        \n",
    "        ax[i][3].title.set_text(\"Predicted\")\n",
    "        ax[i][3].imshow(pred[2*i+1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 avg IoU last image: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-32887502/ipykernel_2858881/530924047.py:138: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.Softmax()(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 avg IoU last image: tensor(0.4574, device='cuda:0')\n",
      "2 avg IoU last image: tensor(0.4653, device='cuda:0')\n",
      "3 avg IoU last image: tensor(0.4577, device='cuda:0')\n",
      "4 avg IoU last image: tensor(0.4670, device='cuda:0')\n",
      "5 avg IoU last image: tensor(0.4904, device='cuda:0')\n",
      "6 avg IoU last image: tensor(0.4735, device='cuda:0')\n",
      "7 avg IoU last image: tensor(0.5057, device='cuda:0')\n",
      "8 avg IoU last image: tensor(0.4519, device='cuda:0')\n",
      "9 avg IoU last image: tensor(0.4836, device='cuda:0')\n",
      "10 avg IoU last image: tensor(0.4626, device='cuda:0')\n",
      "11 avg IoU last image: tensor(0.4873, device='cuda:0')\n",
      "12 avg IoU last image: tensor(0.4642, device='cuda:0')\n",
      "13 avg IoU last image: tensor(0.4897, device='cuda:0')\n",
      "14 avg IoU last image: tensor(0.4799, device='cuda:0')\n",
      "15 avg IoU last image: tensor(0.4602, device='cuda:0')\n",
      "16 avg IoU last image: tensor(0.4715, device='cuda:0')\n",
      "17 avg IoU last image: tensor(0.4650, device='cuda:0')\n",
      "18 avg IoU last image: tensor(0.4733, device='cuda:0')\n",
      "19 avg IoU last image: tensor(0.4630, device='cuda:0')\n",
      "20 avg IoU last image: tensor(0.4831, device='cuda:0')\n",
      "21 avg IoU last image: tensor(0.4758, device='cuda:0')\n",
      "22 avg IoU last image: tensor(0.4695, device='cuda:0')\n",
      "23 avg IoU last image: tensor(0.4729, device='cuda:0')\n",
      "24 avg IoU last image: tensor(0.4725, device='cuda:0')\n",
      "25 avg IoU last image: tensor(0.4993, device='cuda:0')\n",
      "26 avg IoU last image: tensor(0.4455, device='cuda:0')\n",
      "27 avg IoU last image: tensor(0.4421, device='cuda:0')\n",
      "28 avg IoU last image: tensor(0.4567, device='cuda:0')\n",
      "29 avg IoU last image: tensor(0.4814, device='cuda:0')\n",
      "30 avg IoU last image: tensor(0.4824, device='cuda:0')\n",
      "31 avg IoU last image: tensor(0.4898, device='cuda:0')\n",
      "32 avg IoU last image: tensor(0.4663, device='cuda:0')\n",
      "33 avg IoU last image: tensor(0.4818, device='cuda:0')\n",
      "34 avg IoU last image: tensor(0.4507, device='cuda:0')\n",
      "35 avg IoU last image: tensor(0.4709, device='cuda:0')\n",
      "36 avg IoU last image: tensor(0.4955, device='cuda:0')\n",
      "37 avg IoU last image: tensor(0.5062, device='cuda:0')\n",
      "38 avg IoU last image: tensor(0.4490, device='cuda:0')\n",
      "39 avg IoU last image: tensor(0.4877, device='cuda:0')\n",
      "40 avg IoU last image: tensor(0.4509, device='cuda:0')\n",
      "41 avg IoU last image: tensor(0.4511, device='cuda:0')\n",
      "\n",
      "\n",
      "Epoch: 89, Loss: 1412.2901411056519\n",
      ", overall prediction accuracy: 0.9415397644042969\n",
      "\n",
      "\n",
      "Epoch: 89, Prediction accuracy of last frame: 0.9438307285308838\n",
      "Training took time: 0:14:41.286623\n"
     ]
    }
   ],
   "source": [
    "validate_for_11_frames_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_path, transform):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.files = os.listdir(data_path)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        video_path = f\"{self.data_path}/{self.files[i]}\"\n",
    "        frames = []\n",
    "\n",
    "        for j in range(0, 22):\n",
    "            frame = Image.open(f\"{video_path}/image_{j}.png\")\n",
    "            frames.append(self.transform(frame.convert('RGB')))\n",
    "\n",
    "        masks = torch.from_numpy(np.load(f\"{video_path}/mask.npy\"))\n",
    "\n",
    "        return torch.stack(frames), masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = VideoDataset(\"./val\", transforms.ToTensor())\n",
    "val_dataloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    for i, (images, masks) in enumerate(val_dataloader):\n",
    "\n",
    "        B, N, C, H, W = images.shape\n",
    "\n",
    "        images = images.permute(0, 2, 1, 3, 4).reshape(-1, C, H, W).cuda()\n",
    "        masks = masks.reshape(-1, H, W).long().cuda()\n",
    "\n",
    "        pred_masks = predictor(images)\n",
    "\n",
    "        j_ind = jaccard(pred_masks, masks)\n",
    "\n",
    "        print(f\"Val Jaccard: {j_ind}\")\n",
    "        \n",
    "        # print(pred_masks.shape, masks.shape)\n",
    "        \n",
    "        visualize(pred_masks[:22], masks[:22])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 4043746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "train_for_11_frames_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MeanNpyDataset('/scratch/jz3395/val')\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "ckpt = torch.load('1vidconvlstm_predictor80.pth')\n",
    "predictor.load_state_dict(ckpt)\n",
    "predictor.eval()\n",
    "sum_iou=0\n",
    "\n",
    "\n",
    "for i, (mean_tensor, folder_name) in enumerate(test_dataloader):\n",
    "    mean_tensor = mean_tensor.to(torch.float32).permute(0,4,1,2,3)\n",
    "#     mean_tensor=mean_tensor.unsqueeze(1)\n",
    "    prev_mean = mean_tensor[:,:,:11,:,:]\n",
    "    curr_mean=mean_tensor[:,:,11:,:,:]\n",
    "    pred_mask=predictor(prev_mean)\n",
    "    print(prev_mean.shape)\n",
    "    print(\"pred mas\",pred_mask.shape)\n",
    "    for i in range(0,10):\n",
    "        next_mask = curr_mean[:, :, i, :, :].to(device)\n",
    "        next_mask=next_mask.unsqueeze(2)\n",
    "        pred_mask= predictor(next_mask)\n",
    "    print(pred_mask.shape)\n",
    "    for j in range(pred_mask.shape[0]):   \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "        # plot the data on the first subplot\n",
    "        print(pred_mask[j].shape)\n",
    "        seg_pred=pred_mask[j].permute(1,2,0).detach().cpu()\n",
    "        plot_pred=np.argmax(seg_pred, axis=-1)\n",
    "        \n",
    "        ax1.imshow(plot_pred.detach().cpu().numpy().squeeze())\n",
    "        ax1.set_title('predicted')\n",
    "\n",
    "        # plot the data on the second subplot\n",
    "        seg_gt=curr_mean[j, :, -1, :, :].permute(1,2,0).detach().cpu()\n",
    "        plot_gt=np.argmax(seg_gt, axis=-1)\n",
    "        ax2.imshow(plot_gt.cpu().numpy().squeeze())\n",
    "        ax2.set_title(\"GT\")\n",
    "\n",
    "        # adjust the layout of the subplots\n",
    "        fig.tight_layout()\n",
    "\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    break\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tensor=mean_tensor.permute(0,2,3,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask = np.argmax(mean_tensor, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(segmentation_mask[:,21,:,:].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask.permute(0,2,3,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred=pred_mask.permute(0,2,3,1).detach().cpu()\n",
    "seg_pred=np.argmax(p_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg_pred.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the output segmentation map obtained from the semantic segmentation model\n",
    "# output = np.load('output.npy')\n",
    "\n",
    "# Convert the output segmentation map to a tensor and normalize it\n",
    "print(output.shape)\n",
    "output = torch.from_numpy(output.detach().cpu().numpy())\n",
    "print(output.shape)\n",
    "output = F.interpolate(output, size=(160,240), mode='bicubic', align_corners=False)\n",
    "output = F.softmax(output, dim=1)\n",
    "\n",
    "# Find the class with the highest probability for each pixel\n",
    "segmentation_map = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Find the contours of each class in the segmentation map\n",
    "contours = []\n",
    "num_classes=49\n",
    "for i in range(num_classes):\n",
    "    binary_mask = (segmentation_map == i).astype(np.uint8)\n",
    "    cnts, hierarchy = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours.append(cnts)\n",
    "\n",
    "# Draw bounding boxes around the objects of interest for each class\n",
    "boxes = []\n",
    "img = cv2.imread('/scratch/pt2295/Dataset_Student/val/video_1031/image_21.png')\n",
    "for i, cnts in enumerate(contours):\n",
    "    for cnt in cnts:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        boxes.append([x, y, x + w, y + h, i])\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Visualize the result\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0y0wEPJWjQJ"
   },
   "outputs": [],
   "source": [
    "# temp = torch.randn(1, 6, 21, 601, 601)\n",
    "# tout = predictor(temp)\n",
    "def threshold(tensor,thresh):\n",
    "    return torch.where(tensor < thresh, torch.tensor(0.), torch.tensor(1.));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "def for_each_img(attn,w_featmap,h_featmap):\n",
    "#     print(attn.shape)\n",
    "    attn=attn.unsqueeze(0)\n",
    "   \n",
    "    nh = attn.shape[1] # number of head\n",
    "\n",
    "    # we keep only the output patch attention\n",
    "    attn = attn[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "\n",
    "    # we keep only a certain percentage of the mass\n",
    "    val, idx = torch.sort(attn)\n",
    "    val /= torch.sum(val, dim=1, keepdim=True)\n",
    "    cumval = torch.cumsum(val, dim=1)\n",
    "    th_attn = cumval > (1 - 0.6)\n",
    "    idx2 = torch.argsort(idx)\n",
    "    for head in range(nh):\n",
    "        th_attn[head] = th_attn[head][idx2[head]]\n",
    "#     print(w_featmap, h_featmap,th_attn.shape)\n",
    "    th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "    # interpolate\n",
    "    th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "    attn = attn.reshape(nh, w_featmap, h_featmap)\n",
    "    attn = nn.functional.interpolate(attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "    # save attn heatmaps\n",
    "    os.makedirs('output/', exist_ok=True)\n",
    "\n",
    "    # Saving only last attention layer\n",
    "    fname = os.path.join('output/',  \"temp.png\")\n",
    "\n",
    "    plt.imsave(\n",
    "        fname=fname,\n",
    "        arr=sum(attn[i] * 1/attn.shape[0] for i in range(attn.shape[0])),\n",
    "        cmap=\"inferno\",\n",
    "        format=\"jpg\"\n",
    "    )\n",
    "    img = Image.open('/scratch/pt2295/predictor/output/temp.png')\n",
    "\n",
    "    # convert the image to grayscale using torchvision.transforms\n",
    "    gray_transform = transforms.Grayscale()\n",
    "    gray_img = gray_transform(img)\n",
    "    t=transforms.ToTensor()(gray_img)\n",
    "    p=threshold(t,att_thresh)\n",
    "\n",
    "    \n",
    "    return p\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_att(tensor,w,h):\n",
    "#     print(tensor.shape)\n",
    "   \n",
    "    temp=[]\n",
    "    for i in range(tensor.shape[0]):\n",
    "        temp.append(for_each_img(tensor[i],w,h))\n",
    "    return torch.cat(temp,dim=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_thresh=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkv0Uq9iWjQJ"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def threshold(tensor,thresh):\n",
    "    return torch.where(tensor < thresh, torch.tensor(0.), torch.tensor(1.));\n",
    "def train_for_11_frames_input():\n",
    "    num_epochs = 50\n",
    "\n",
    "    encoder.eval()\n",
    "    predictor.train()\n",
    "\n",
    "    t1 = datetime.now()\n",
    "    ctr=0\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (batch, _, _) in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "            print(i, end=\", \")\n",
    "            w, h = batch.shape[1] - batch.shape[1] % 8, batch.shape[2] - batch.shape[2] % 8\n",
    "            # img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "            w_featmap = batch.shape[-2] // 8\n",
    "            h_featmap = batch.shape[-1] // 8   \n",
    "            with torch.no_grad():\n",
    "                attentions = encoder.module.student_backbone.module.get_last_selfattention(batch.cuda())\n",
    "            attentions=transform_att(attentions,w_featmap,h_featmap)\n",
    "            print(attentions.shape)\n",
    "            attentions=attentions.unsqueeze(1)\n",
    "\n",
    "            \n",
    "            \n",
    "          \n",
    "            \n",
    "            N, C, H, W = attentions.shape\n",
    "        \n",
    "            \n",
    "            B = int(N // num_frames)\n",
    "            \n",
    "            attentions = attentions.view(B, num_frames, C, H, W)\n",
    "           \n",
    "\n",
    "            prev_attns = attentions[:, :11].permute(0,2,1,3,4)\n",
    "         \n",
    "            curr_attns = attentions[:, 11:]\n",
    "   \n",
    "            pred_attn= predictor(prev_attns)\n",
    "\n",
    "\n",
    "            for i in range(0, 10):\n",
    "\n",
    "                loss = criterion(pred_attn, curr_attns[:,i].to(device))\n",
    "                \n",
    "\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                next_attn = curr_attns[:,i].unsqueeze(1).permute(0,2,1,3,4)\n",
    "\n",
    "                pred_attn= predictor(next_attn.to(device))\n",
    "\n",
    "\n",
    "            loss = criterion(pred_attn, curr_attns[:,-1].to(device))\n",
    "            print(pred_attn.shape,\"predic shape\",curr_attns[:,-1].shape)\n",
    "            visualise(threshold(pred_attn[0,:,:,:],att_thresh),is_pred=True)\n",
    "            visualise(curr_attns[:,-1][0,:,:,:].to(device))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        ctr+=1\n",
    "\n",
    "        torch.save(predictor.state_dict(), 'convlstm_predictor'+str(ctr)+'.pth')\n",
    "\n",
    "        print(f\"\\n\\nEpoch: {e}, Loss: {total_loss}\\n\")\n",
    "\n",
    "    print(f\"Training took time: {datetime.now() - t1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEE6VHDBWjQJ",
    "outputId": "f6bdd59a-0e76-46f0-dc1f-01a52eaa15cb"
   },
   "outputs": [],
   "source": [
    "# # train_for_21_frames_input()\n",
    "# 16, 1, 10, 64, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScxZWEwqWjQK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_for_11_frames_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D74DNDzPWjQK"
   },
   "outputs": [],
   "source": [
    "predictor.module.first_layer.module.convLSTMcell.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1XI5YP8WjQK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def my_save_image(attn, pred, ctr,is_attn=False):\n",
    "\n",
    "    print(attn.shape)\n",
    "    nh = attn.shape[1] # number of head\n",
    "\n",
    "    # we keep only the output patch attention\n",
    "    attn = attn[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "\n",
    "    # we keep only a certain percentage of the mass\n",
    "    val, idx = torch.sort(attn)\n",
    "    val /= torch.sum(val, dim=1, keepdim=True)\n",
    "    cumval = torch.cumsum(val, dim=1)\n",
    "    th_attn = cumval > (1 - 0.6)\n",
    "    idx2 = torch.argsort(idx)\n",
    "    for head in range(nh):\n",
    "        th_attn[head] = th_attn[head][idx2[head]]\n",
    "    print(w_featmap, h_featmap,th_attn.shape)\n",
    "    th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "    # interpolate\n",
    "    th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "    attn = attn.reshape(nh, w_featmap, h_featmap)\n",
    "    attn = nn.functional.interpolate(attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "    # save attn heatmaps\n",
    "    os.makedirs('output/', exist_ok=True)\n",
    "\n",
    "    # Saving only last attention layer\n",
    "    fname = os.path.join('output/', \"attn-\" + str(ctr) + pred + \".png\")\n",
    "\n",
    "    plt.imsave(\n",
    "        fname=fname,\n",
    "        arr=sum(attn[i] * 1/attn.shape[0] for i in range(attn.shape[0])),\n",
    "        cmap=\"inferno\",\n",
    "        format=\"jpg\"\n",
    "    )\n",
    "    print(f\"{fname} saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(p,is_pred=False):\n",
    "    np_array = p.detach().cpu().numpy().reshape(160, 240)\n",
    "\n",
    "    # convert numpy array to PIL Image and convert to grayscale\n",
    "    pil_image = Image.fromarray(np.uint8(np_array * 255))\n",
    "    gray_image = pil_image.convert('L')\n",
    "\n",
    "    # save grayscale PIL Image to disk\n",
    "    if(is_pred):\n",
    "        gray_image.save('/scratch/pt2295/predictor/output/gray_image_pred.png')\n",
    "    else:\n",
    "        gray_image.save('/scratch/pt2295/predictor/output/gray_image_att.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMROwCHFWjQK"
   },
   "outputs": [],
   "source": [
    "# from torchvision.utils import save_image\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "ckpt = torch.load('convlstm_predictor49.pth')\n",
    "predictor.load_state_dict(ckpt)\n",
    "\n",
    "test_dataset = LightlyDataset(input_dir=\"/scratch/pt2295/proj/test\", transform=transform)\n",
    "test_dataset.transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=num_frames,   # batch size 1 for inference\n",
    "    shuffle=False,  # don't shuffle data during inference\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "predictor.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "ctr = 0\n",
    "for img, label, fnames in test_dataloader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(fnames)\n",
    "        img = img.cuda()\n",
    "        print(img.shape)\n",
    "        w, h = img.shape[1] - img.shape[1] % 8, img.shape[2] - img.shape[2] % 8\n",
    "        # img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "        w_featmap = img.shape[-2] // 8\n",
    "        h_featmap = img.shape[-1] // 8\n",
    "\n",
    "        attentions = encoder.module.student_backbone.module.get_last_selfattention(img.cuda())\n",
    "        temp=attentions\n",
    "        attentions=transform_att(attentions,w_featmap,h_featmap)\n",
    "        attentions=attentions.unsqueeze(1)\n",
    "\n",
    "\n",
    "        N, C, H, W = attentions.shape\n",
    "\n",
    "        prev_attns = attentions[:11].view(1, 11, C, H, W).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "\n",
    "        pred_attn = predictor(prev_attns)\n",
    "\n",
    "        for i in range(11, 21):\n",
    "\n",
    "            next_attn = attentions[i].view(1, 1, C, H, W).permute(0,2,1,3,4)\n",
    "\n",
    "            pred_attn= predictor(next_attn)\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        correct += torch.sum(pred_attn.view(1, -1) == attentions[-1].view(1, -1).to(device))\n",
    "\n",
    "        total += 6*601*601\n",
    "\n",
    "        my_save_image(attentions[-1], \"_3321_o\", ctr)\n",
    "        my_save_image(pred_attn, \"_3321_p\", ctr)\n",
    "        print(\"##############\")\n",
    "#         print(attentions[-1])\n",
    "        print(\"*************\")\n",
    "#         print(pred_attn)\n",
    "        \n",
    "        print(\"how many equal\",(threshold(pred_attn,0.2).cuda() == attentions[-1].squeeze(0).cuda()).sum().item())\n",
    "        print(\"1 in attention\",(attentions[-1] == 1).sum().item())\n",
    "        print(\"how many in pred 1\",(threshold(pred_attn,0.2) == 1).sum().item())\n",
    "        print(\"mae\")\n",
    "\n",
    "        ctr += 1\n",
    "        \n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.utils as vutils\n",
    "# vutils.save_image(threshold(pred_attn), 'my_predsttn.png', normalize=True, range=(0, 255))\n",
    "# vutils.save_image(attentions[-1].squeeze(0), 'my_attn.png', normalize=True, range=(0, 255))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vutils.save_image(temp[-1], 'my_tttttempattn.png', normalize=True, range=(0, 1), nrow=1, pad_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[-1].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_save_image(temp[-1].unsqueeze(0),\"blah\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.abs(threshold(pred_attn) - attentions[-1].squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f81d2-CWjQK",
    "outputId": "238f7b4d-7176-4baa-989a-4ae41b4bf6fc"
   },
   "outputs": [],
   "source": [
    "# attentions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_attn.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(pred_attn[:,:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgJrHiRmWjQL",
    "outputId": "17c705d3-bbb8-4985-8ac3-eca87cdf3e2c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxW21fk-WjQL",
    "outputId": "dacff8b3-9c64-48ad-e16d-ee741b68d8a0"
   },
   "outputs": [],
   "source": [
    "# torch.sigmoid(pred_attn[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zp6AXZVJWjQL"
   },
   "outputs": [],
   "source": [
    "loss = criterion(pred_attn, torch.sigmoid(attentions[-1].unsqueeze(0)).to(device))\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIbfbC98WjQL"
   },
   "outputs": [],
   "source": [
    "pred_attn.shape, attentions[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agk0ctLSWjQL"
   },
   "outputs": [],
   "source": [
    "datetime.now() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPcuGLARWjQL"
   },
   "outputs": [],
   "source": [
    "9714 / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAqiVf6dWjQL"
   },
   "outputs": [],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJNgtL2FWjQL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# create a grayscale tensor\n",
    "x = torch.rand(1, 601, 601)\n",
    "\n",
    "# save tensor as a grayscale image\n",
    "vutils.save_image(x, 'my_tensor_image.png', normalize=True, range=(0, 1), nrow=1, pad_value=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# define the image file path\n",
    "img_file = '/scratch/pt2295/predictor/output/attn-2blah.png'\n",
    "\n",
    "# create a transformation to convert the image to a tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the image as a tensor\n",
    "img_tensor = transform(Image.open(img_file))\n",
    "\n",
    "# print the tensor shape\n",
    "print(img_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# load the image as a PIL Image object\n",
    "img = Image.open('/scratch/pt2295/predictor/output/attn-2blah.png')\n",
    "\n",
    "# convert the image to grayscale using torchvision.transforms\n",
    "gray_transform = transforms.Grayscale()\n",
    "gray_img = gray_transform(img)\n",
    "\n",
    "# save the grayscale image as a file\n",
    "gray_img.save('/scratch/pt2295/predictor/output/att2blah_gray.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=transforms.ToTensor()(gray_img)\n",
    "torch.sum(t > 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=threshold(t,0.2)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = p.numpy().reshape(160, 240)\n",
    "\n",
    "# convert numpy array to PIL Image and convert to grayscale\n",
    "pil_image = Image.fromarray(np.uint8(np_array * 255))\n",
    "gray_image = pil_image.convert('L')\n",
    "\n",
    "# save grayscale PIL Image to disk\n",
    "gray_image.save('/scratch/pt2295/predictor/output/grayscale_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(t.flatten().numpy(), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# load the image as a PIL Image object\n",
    "img = Image.open('/scratch/pt2295/predictor/output/attn-2blah.png')\n",
    "\n",
    "# convert the image to grayscale using torchvision.transforms\n",
    "gray_transform = transforms.Grayscale()\n",
    "\n",
    "gray_img = gray_transform(img)\n",
    "\n",
    "# save the grayscale image as a file\n",
    "gray_img.save('/scratch/pt2295/predictor/output/ttatt2blah_gray.jpg')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
