{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7AG7e7p7FapE"},"outputs":[],"source":["import os\n","import numpy as np\n","\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","from PIL import Image, ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","from datetime import datetime\n","from torchmetrics import JaccardIndex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7uGIE15FapF","outputId":"35ee84cc-5213-415f-c5b2-abe66165bb41"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MT8W3VmxFapG"},"outputs":[],"source":["batch_size = 4\n","# num_frames = 22\n","num_classes = 49\n","input_size = (160, 240)\n","\n","device_ids = [i for i in range(torch.torch.cuda.device_count())]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Sv1n0cPFapG","outputId":"9089d03e-7fa8-41bf-f4d2-95c4ed1e07e8"},"outputs":[{"data":{"text/plain":["[0, 1]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["device_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZXwVLc6FapG"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class VideoDataset(Dataset):\n","    def __init__(self, data_path, transform):\n","        \n","        self.data_path = data_path\n","        \n","        self.files = os.listdir(data_path)\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.files)\n","    \n","    def __getitem__(self, i):\n","        \n","        video_path = f\"{self.data_path}/{self.files[i]}\"\n","        frames = []\n","\n","        for j in range(0, 22):\n","            frame = Image.open(f\"{video_path}/image_{j}.png\")\n","            frames.append(self.transform(frame.convert('RGB')))\n","\n","        masks = torch.from_numpy(np.load(f\"{video_path}/mask.npy\"))\n","\n","        return torch.stack(frames), masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWMKn0M0FapH"},"outputs":[],"source":["trainset = VideoDataset(\"./Dataset_Student/train\", transforms.ToTensor())\n","valset = VideoDataset(\"./Dataset_Student/val\", transforms.ToTensor())\n","\n","train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False)\n","val_dataloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4QhRy1gFapH"},"outputs":[],"source":["from pytorch_unet.unet.unet_parts import Down, Up, OutConv, DoubleConv\n","\n","class ShallowUNet(nn.Module):\n","    '''\n","        Input dim: (batch_size*num_frames, num_channels, image_height, image_width)\n","        output dim: (batch_size*num_frames, num_classes, image_height, image_width)\n","    '''\n","    def __init__(self, n_channels, n_classes=49, bilinear=False):\n","        \n","        super(ShallowUNet, self).__init__()\n","        \n","        factor = 2 if bilinear else 1\n","        \n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","        \n","        self.inc = DoubleConv(n_channels, 64)\n","        \n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.down4 = Down(512, 1024 // factor)\n","        \n","        self.up1 = Up(1024, 512 // factor, bilinear)\n","        self.up2 = Up(512, 256, bilinear)\n","        self.up3 = Up(256, 128, bilinear)\n","        self.up4 = Up(128, 64, bilinear)\n","        \n","        self.outc = OutConv(64, n_classes)\n","\n","    def forward(self, x):\n","        \n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","                \n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        \n","        logits = self.outc(x)\n","        \n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhsU3GDrFapH"},"outputs":[],"source":["def decoder_output_to_mask(output, mask_values=list(range(49))):\n","    \n","    mask = output.argmax(dim=1)\n","    out_tensor=mask[0].squeeze()\n","\n","    mask = mask[0].long().squeeze().numpy()\n","    out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.uint8)\n","    \n","    for i, v in enumerate(mask_values):\n","        out[mask == i] = v\n","        \n","    return Image.fromarray(out),output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgpML8iAFapI"},"outputs":[],"source":["model = nn.DataParallel(ShallowUNet(3).cuda(), device_ids=device_ids).cuda()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score\n","grad_scaler = torch.cuda.amp.GradScaler(enabled=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvYCgrn-FapI"},"outputs":[],"source":["# tin = torch.randn(1, 22, 3, 160, 240)\n","# tin = tin.permute(0, 2, 1, 3, 4)  # Reshape to (batch_size, num_channels, num_frames, height, width)\n","# tin = tin.reshape(-1, 3, 160, 240)\n","\n","# tout = model(tin)\n","# tout.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-mIZP6WFapI"},"outputs":[],"source":["from pytorch_unet.utils.dice_score import dice_loss\n","\n","def train_model(save_ckpt):\n","\n","    model.train()\n","\n","    correct, total, total_loss = 0, 0, 0\n","    j_ind = 0 \n","\n","    for i, (images, masks) in enumerate(train_dataloader):\n","\n","        optimizer.zero_grad()\n","\n","        if( i % 10 == 0):\n","            print(i, end=\"  \")\n","\n","        B, N, C, H, W = images.shape\n","\n","        images = images.permute(0, 2, 1, 3, 4).reshape(-1, C, H, W).cuda()\n","        masks = masks.reshape(-1, H, W).long().cuda()\n","\n","        pred_masks = model(images)\n","\n","        loss = criterion(pred_masks, masks)\n","        \n","        loss += dice_loss(\n","            F.softmax(pred_masks, dim=1).float(),\n","            F.one_hot(masks, model.module.n_classes).permute(0, 3, 1, 2).float(),\n","            multiclass=True\n","        )\n","        \n","        grad_scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        grad_scaler.step(optimizer)\n","        grad_scaler.update()\n","\n","#         j_ind = jaccard(pred_masks, masks)\n","\n","        b, h, w = masks.shape\n","        correct += torch.sum(pred_masks.argmax(dim=1).squeeze() == masks)\n","        total += b*h*w\n","\n","#         loss.backward()\n","#         optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    if(save_ckpt):\n","        torch.save(model.state_dict(), 'unet_segmentor.pth')\n","\n","    print(f\"\\n\\nEpoch: {e+1}, Loss: {total_loss}, Correct: {correct/total}\\n\")\n","\n","\n","def evaluate_model():\n","    \n","    correct, total, val_loss = 0, 0, 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","\n","        for i, (images, masks) in enumerate(val_dataloader):\n","            \n","            if i == 50:\n","                break\n","            \n","            B, N, C, H, W = images.shape\n","\n","            images = images.permute(0, 2, 1, 3, 4).reshape(-1, C, H, W).cuda()\n","            masks = masks.reshape(-1, 1, H, W).long().cuda()\n","\n","            pred_masks = model(images)\n","\n","            val_loss += F.cross_entropy(pred_masks, masks.squeeze(1))\n","            \n","            correct += torch.sum(pred_masks.argmax(dim=1).squeeze() == masks)\n","            total += B * N * H * W\n","\n","        print(f\"Val Loss: {val_loss}, Accuracy: {correct/total}\\n\")\n","        \n","    mask, _ = decoder_output_to_mask(pred_masks.cpu())\n","    return mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCR-HqrRFapI","outputId":"9f8aa085-dc1f-4335-a24d-c2034d76b207"},"outputs":[{"name":"stdout","output_type":"stream","text":["0  10  20  30  40  50  60  70  80  "]}],"source":["num_epochs = 10\n","\n","jaccard = JaccardIndex(task=\"multiclass\", num_classes=49)\n","\n","t1 = datetime.now()\n","\n","for e in range(num_epochs):\n","    train_model(True)\n","    \n","    if e%5 == 0:\n","        print(evaluate_model())\n","\n","print(evaluate_model())\n","\n","print(f\"Training took time: {datetime.now() - t1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNQKrAUFFapI","outputId":"bd9ad9f2-ebb9-4b87-8bc9-d4dd5ecbcf04"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPAAAACgCAAAAAAXsL65AAABH0lEQVR4nO3YQWoCQRAF0J8QBgNCkCy8QO5/LxcSXI0IQhYtickupSgl721GhW76U1XOaAIAAAAAAAAAAAAAAAAAAAAAAMADebr3Af5tleSzvvzlage5jcXrhRs8X+UYt3OWd13aoFvgYZFkfVhVljab4Y/tuE7JIaVZ7jTDi+Nym2S6aJNOLX1cjutmk1Hgik4tPWZ2FHjkLbR0pwon+d3Q74X1nQLPfz/YFjbpFPhMdYJ7Bd7POUU95a08YXa6LSXz2/m70hN1pwr/mKZMxV8QnQIvzl5X87YK/MumuK7Rg8co8GmKq3kbVrh+R0rSKvB+nyS7ObtLdmnU0kmymsc/AFO5pZsF/v6q3t/3FAAAAAAAAAAAAAAAAADAo/gCvScjxyzz5R8AAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=240x160 at 0x14ADDCD48AC0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["mask, _ = decoder_output_to_mask(pred_masks.cpu())\n","mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8sy4ARYFapI"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}